---
title: "MA415 Final Project"
author: "Xiaoqian Xue"
date: "12/4/2017"
output: ioslides_presentation
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```

## R Markdown

This is an R Markdown presentation. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see <http://rmarkdown.rstudio.com>.

When you click the **Knit** button a document will be generated that includes both content as well as the output of any embedded R code chunks within the document.

## Slide with Bullets

- Bullet 1
- Bullet 2
- Bullet 3

## Slide with R Output

## Twitter Setup
```{r cars, echo = TRUE}
library(twitteR)


api_key <- "zwmo2nKieK0RGVoFMP9KfGci9"
api_secret <- "Az0OyiiUDyWJlCwnZtEBJxRKLNJIZ4FqOPyUvyDNqUM7Ko2lUr"
access_token <- "2809402781-RQuIDNFLL1dg5i3xETd1W3t8Kol9ZVqufMrgsRf"
access_token_secret <- "q5s33WockcM1JdwLyxJmQP9zGv3B0ilzYXmnYF8TMAN0q"
setup_twitter_oauth(api_key, api_secret, access_token, access_token_secret)
# [1] "Using direct authentication"
# Use a local file ('.httr-oauth'), to cache OAuth access credentials between R sessions?
# 
# 1: Yes
# 2: No
# 
# Selection: 1
# Adding .httr-oauth to .gitignore
```

## Acquire Starbucks Official Tweets and Tweets from other users when they make hashtags of Starbucks and Text Cleaning
```{r, echo = TRUE}
## retrive Starbucks Official tweets  and tweets from other users from Twitter 
tweets1 <- userTimeline("Starbucks", n = 1000)
tweets2 <- searchTwitter('#starbucks', n= 1000, lang = 'en',resultType = 'recent')
# convert tweets to a data frame
tweets1df <- twListToDF(tweets1)
tweets2df <- twListToDF(tweets2)
# tweet #100
tweets1df[100, c("id", "created", "screenName", "replyToSN","favoriteCount", "retweetCount", "longitude", "latitude", "text")]
tweets2df[100, c("id", "created", "screenName", "replyToSN","favoriteCount", "retweetCount", "longitude", "latitude", "text")]
# print tweet #100
writeLines(strwrap(tweets1df$text[100], 60))
writeLines(strwrap(tweets2df$text[100], 60))
library(tm)
# build a corpus, and specify the source to be character vectors 
starbucks1 <- Corpus(VectorSource(tweets1df$text))
starbucks2 <- Corpus(VectorSource(tweets2df$text))
# remove URLs
removeURL <- function(x) gsub("http[^[:space:]]*", "", x)
starbucks1 <- tm_map(starbucks1, content_transformer(removeURL))
starbucks2 <- tm_map(starbucks2, content_transformer(removeURL))
# remove anything other than English letters or space remove
removeNumPunct <- function(x) gsub("[^[:alpha:][:space:]]*", "", x) 
starbucks1 <- tm_map(starbucks1, content_transformer(removeNumPunct))
starbucks2 <- tm_map(starbucks2, content_transformer(removeNumPunct))
# remove stopwords
myStopwords <- c(setdiff(stopwords('english'), c("r", "big")),
                     "use", "see", "used", "via", "amp")
starbucks1 <- tm_map(starbucks1, removeWords, myStopwords)
starbucks2 <- tm_map(starbucks2, removeWords, myStopwords)
# remove extra whitespace
starbucks1 <- tm_map(starbucks1, stripWhitespace)
starbucks2 <- tm_map(starbucks2, stripWhitespace)
# convert to lower case
starbucks1 <- tm_map(starbucks1, content_transformer(tolower))
starbucks2 <- tm_map(starbucks2, content_transformer(tolower))
```
## Number of Tweets in each hour
```{r}
count <- function(x){
  b <- vector(mode = "numeric",length=0)
  for (i in (1:length(x))){
    a = strsplit(as.character(x[i]),' ')
    a = a[[1]][2]
    a = strsplit(a,":")
    b = c(b,a[[1]][1])
    
  }
  return(b)
  
  }
hour1 <-count(tweets1df$created)
hour2 <-count(tweets2df$created)
tweets1df$hour1 <- hour1
tweets2df$hour2 <- hour2
library(ggplot2)
ggplot(tweets1df,aes(x=hour1))+geom_bar(aes(y=(..count..)))
ggplot(tweets2df,aes(x=hour2))+geom_bar(aes(y=(..count..)))


```

## Inspect Frequent Words from Starbucks Official Tweets and Starbucks Followers Tweets
```{r, echo = TRUE}
# Build Term Document Matrix
tdm1 <- TermDocumentMatrix(starbucks1,control = list(wordLengths = c(1, Inf)))
tdm2 <- TermDocumentMatrix(starbucks2,control = list(wordLengths = c(1, Inf)))
tdm1
tdm2
# inspect frequent words
freq.terms <- findFreqTerms(tdm1, lowfreq = 30)
freq.terms <- findFreqTerms(tdm2, lowfreq = 30)
term1.freq <- rowSums(as.matrix(tdm1))
term2.freq <- rowSums(as.matrix(tdm2))
term1.freq <- subset(term1.freq, term1.freq >= 30)
term2.freq <- subset(term2.freq, term2.freq >= 30)
df1 <- data.frame(term = names(term1.freq), freq = term1.freq)
df2 <- data.frame(term = names(term2.freq), freq = term2.freq)
library(ggplot2)
ggplot(df1, aes(x=term, y=freq)) + geom_bar(stat = "identity") + xlab ("Terms") + ylab("Count") + coord_flip() + theme(axis.text=element_text(size=7))
ggplot(df2, aes(x=term, y=freq)) + geom_bar(stat = "identity") + xlab ("Terms") + ylab("Count") + coord_flip() + theme(axis.text=element_text(size=7))

```
## Wordcloud
```{r,echo = TRUE}
m1 <- as.matrix(tdm1)
word1.freq <- sort(rowSums(m1), decreasing = T)
library(RColorBrewer)
pal <- brewer.pal(9, "Dark2")
library(wordcloud)
wordcloud(words=names(word1.freq), freq=word.freq,min.freq=10, random.order = F, colors = pal)
m2 <- as.matrix(tdm2)
word2.freq <- sort(rowSums(m2), decreasing = T)
wordcloud(words=names(word2.freq), freq=word.freq,min.freq=10, random.order = F, colors = pal)

```
## Compare Topics
```{r,echo = TRUE}
# Topic Modelling
dtm1 <- as.DocumentTermMatrix(tdm1)
dtm2 <- as.DocumentTermMatrix(tdm2)
library(topicmodels)
lda1 <- LDA(dtm1, k = 8)
lda2 <- LDA(dtm2, k = 8)
term1 <- terms(lda1,7)
term2 <- terms(lda2,7)
term1 <- apply(term1,MARGIN = 2, paste, collapse = ", ")
term2 <- apply(term2,MARGIN = 2, paste, collapse = ", ")
term1
term2
library(data.table)
topics1 <- topics(lda1)
topics2 <- topics(lda2)
topics1 <- data.frame(date=as.IDate(tweets1df$created), topic = topics1)
topics2 <- data.frame(date=as.IDate(tweets2df$created), topic = topics2)
ggplot(topics1, aes(date, fill = term1[topic])) +
  geom_density(position = "stack")
ggplot(topics2, aes(date, fill = term2[topic])) +
  geom_density(position = "stack")
```

```{r}
# Sentiment Analysis: use sentiment text analysis package from 
library(devtools)
require(devtools)
install_github("sentiment140","okugami79")
library(sentiment)
library(RCurl)
library(bitops)
library(rjson)
library(plyr)
sentiments1 <- sentiment(tweets1df$text)
sentiments2 <- sentiment(tweets2df$text)
table(sentiments1$polarity)
table(sentiments2$polarity)

# sentiment plot
sentiments1$score <- 0 
sentiments1$score[sentiments1$polarity == "positive"] <- 1
sentiments1$score[sentiments1$polarity == "negative"] <- -1
sentiments1$date <-as.IDate(tweets1df$created)
sentiments2$score <- 0 
sentiments2$score[sentiments2$polarity == "positive"] <- 1
sentiments2$score[sentiments2$polarity == "negative"] <- -1
sentiments2$date <-as.IDate(tweets2df$created)
result1 <- aggregate (score~date, data = sentiments1, sum)
result2 <- aggregate (score~date, data = sentiments2, sum)
plot(result1, type = "l")
plot(result2, type = "l")

```


```{r, echo=TRUE}
# United States
library(dismo)
library(maps)
library(ggplot2)
library(twitteR)
library(RgoogleMaps)
library(ggmap)
map('world')
points(tweets2.df$longitude,tweets2.df$latitude,pch=20, cex =1, col="red")

```








```{r,echo=TRUE}
start<- getUser("Starbucks")
friends.object<-lookupUsers(start$getFriends(500))
follower.object<-lookupUsers(start$getFollowers(500))
```
## Slide with Plot

```{r pressure}
library(ggmap)
library(ggplot2)
ggplot(data = trendingTweets.df, aes(x = created)) +
  geom_histogram(aes(fill = ..count..)) +
  theme(legend.position = "none") +
  xlab("Time") + ylab("Number of tweets") +
  scale_fill_gradient(low = "midnightblue", high = "aquamarine4")



```

